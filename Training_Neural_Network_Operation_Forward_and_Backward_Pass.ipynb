{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How a Neural Network toolkit works\n",
    "\n",
    "Tensorflow is the toolkit of primitives that underlies Keras.\n",
    "\n",
    "It is what powers training and computation in Neural Networks.\n",
    "\n",
    "Although it might seem mysterious, it (and similar toolkits) is based on a very simple concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is pseudo-code for the *training loop*\n",
    "- The part of the Keras framework that implements `fit`\n",
    "- It solves for the optimal weights $\\W^*$ that minimize the Loss function\n",
    "- Pre-Keras, the user coded this loop for each problem\n",
    "\n",
    "It is nothing more than Gradient Descent."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "initialize(W)\n",
    "\n",
    "# Training loop to implement mini-batch SGD\n",
    "for epoch in range(n_epochs):`\n",
    "    for X_batch, y_batch in next_batch(X_train, y_train, batch_size, shuffle=True):\n",
    "        # Forward pass\n",
    "        y = NN(X_batch)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = loss_fn(y, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = gradient(loss, W)\n",
    "        \n",
    "        # Update \n",
    "        W = W - grads * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We process all the training examples once per epoch\n",
    "- The epoch is divided into *mini-batches*: disjoint subsets of training examples\n",
    "- The estimate of the weights is updated in each epoch\n",
    "- We do this for many epochs, until the Loss function no longer decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each epoch consists of two phases\n",
    "- A Forward Pass in which inputs are mapped into predictions, for each example in the minibatch\n",
    "    - An Average Loss is computed over all examples in the minibatch\n",
    "- A Backward Pass in which gradients of the Average Loss are computed\n",
    "    - And used to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Forward and Backward API\n",
    "\n",
    "There is a clever \"trick\" that facilitates\n",
    "- Computation of predictions (Forward Pass)\n",
    "- Computation of analytical derivatives (Backward Pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Each atomic operation is implmented by an Object-Oriented Class**\n",
    "\n",
    "The class implements methods\n",
    "- `forward` for the Forward Pass\n",
    "- `backward` for the Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This trick is repeated many times, for each atomic operation.\n",
    "\n",
    "That's all there is to it: Consistent application of a simple trick !\n",
    "\n",
    "Let's illustrate using the Multiplication operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Forward Pass\n",
    "\n",
    "The essential part of the Forward Pass is computing layer $\\\\l$'s output $\\y_\\llp$ from\n",
    "the layer's input $\\y_{(\\ll-1)}$ and the layer's weights $\\W_{(\\ll)}$.\n",
    "\n",
    "$$\n",
    "\\y_{(\\ll)} = a_{(\\ll)}( f_{(\\ll)}( \\y_{(\\ll-1)}, \\W_{(\\ll)})\n",
    "$$\n",
    "\n",
    "For simplicity of presentation, we will temporarily assume that the activation $a_\\llp$ is the identity\n",
    "function.\n",
    "\n",
    "(Without loss of generality, we can implement the activation as a separate layer that also obey's\n",
    "the per layer logic we are about to present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the atomic operation of multiplication\n",
    "`x * y`\n",
    "\n",
    "We define a class `MultiplyLayer`\n",
    "- derived from parent class `Layer`, which requires the `forward` and `backward` methods\n",
    "\n",
    "Here is the code for the Forward Pass"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " class MultiplyLayer(Layer):\n",
    "    \"\"\"\n",
    "    A layer that multiplies its two inputs (x,y)\n",
    "    \"\"\"\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        The forward pass: compute the product of x, y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x, y: ndarrays\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        z: ndarray that is the product of x and y\n",
    "        \"\"\"\n",
    "         "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "        # Compute the product\n",
    "        z = x * y\n",
    "        \n",
    "        # Remember the two inputs: we will need to take derivatives with respect to each\n",
    "        self.x, self.y = x, y\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not surprisingly\n",
    "- The key statement is the one that multiplies the two inputs\n",
    "- And returns the product\n",
    "\n",
    "Just as you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But also notice that we are saving the two multiplicands (x and y).\n",
    "\n",
    "We will need them for the Backward Pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inside the Backward Pass\n",
    "\n",
    "The job of the Backward Pass is\n",
    "- To take the Loss gradient $\\loss'_\\llp$ for the layer\n",
    "- Compute the Loss gradient $\\loss'_{(\\ll-1)}$ to \"flow backwards\" to the previous layer\n",
    "- Compute the Local gradients\n",
    "- Obtain the derivative with respect to $\\W_\\llp$, the layer's weights, using the Loss and Local gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is, the derivative of the Loss with respect to the layer's weights\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp}\n",
    "$$\n",
    "\n",
    "is computed via the Chain Rules as\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Here is the code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   def backward(self, dz): \n",
    "        \"\"\"\n",
    "        The backward pass: \n",
    "        - update the derivative of the loss function (to the derivative wrt the output of the prior layer)\n",
    "        - compute the derivatives of the loss function with respect to each input\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        dz: scalar.  \"loss gradient\": dL/dz: \n",
    "        - The derivative of the loss wrt the output (z) of this layer\n",
    "       \n",
    "        Returns\n",
    "        --------\n",
    "        [dx, dy]: \"local gradients\" wrt inputs [x,y]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " \n",
    "\t# Compute loss gradient of this layer, given that of successor layer\n",
    "\t# Loss gradient this layer = Upstream Loss gradient * local gradient \n",
    "        dL/dx = dL/dz * dz/dx\n",
    "        dL/dy = dL/dz * dz/dt\n",
    "        \n",
    "        Since this layer's operation  is multiplication, z = x*y\n",
    "        - \"local gradients\" are dz/dx = y, dz/dy = x\n",
    "        \n",
    "        dz is given as input\n",
    "        \"\"\"\n",
    "        \n",
    "        local_grad_x = self.y\n",
    "        local_grad_y = self.x\n",
    "         \n",
    "        dx = local_grad_x * dz\n",
    "        dy = local_grad_y * dz\n",
    "        \n",
    "        return [dx, dy]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The `backward` method\n",
    "- Takes the loss gradient $\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial y_\\llp}$ as input via formal parameter `dz`\n",
    "  - Variable `dz` denotes $\\frac{\\partial \\loss}{\\partial z}$. the derivative of the loss with respect to `z` \n",
    "    - Which is the loss gradient $\\frac{\\partial \\loss}{\\partial y_\\llp}$\n",
    "    - Since variable `z` is the name for  $\\y_\\llp$ \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Computes the local gradients $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "$$\n",
    "    \\begin{array}[lll]\\\\\n",
    "    \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} & = &[ \\frac{\\partial \\y_\\llp}{\\partial x},  \\frac{\\partial \\y_\\llp}{\\partial y}] & \\text{Since } \\y_{(\\ll-1)} = [x,y]\\\\\n",
    "    & = & [ \\frac{\\partial z}{\\partial x},  \\frac{\\partial z}{\\partial y}] & \\text{Since } z = y_\\llp \\\\\n",
    "    & = & [ \\frac{\\partial (x*y)}{\\partial x},  \\frac{\\partial (x*y)}{\\partial y}] & \\text{Since } z = x*y \\\\\n",
    "    & = & [ y,  x] & \\text{Since } z = x*y \\\\\n",
    "    \\end{array}\n",
    "    $$\n",
    "    \n",
    "- `local_grad_x, local_grad_y` are the variables that store the local gradients    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Multiplies the loss gradient $\\loss'_\\llp$ (stored in variable `dz`)\n",
    "- By the local gradients $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$ (stored in variables `local_grad_x, local_grad_y`)\n",
    "- To compute the product which is $\\loss'_{(\\ll-1)}$\n",
    "- Returned as `[ dx, dy ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the `backward` method flows the loss gradient \"backwards\" one layer\n",
    "- And facilitates the computation of \n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$\n",
    "- In the multiply layer, there are no weights $\\W_\\llp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now you can see why the `forward` method stored the multiplicands `x, y`\n",
    "- They were needed as\n",
    "   $[ y, x ] = [ \\frac{\\partial (x*y)}{\\partial x},  \\frac{\\partial (x*y)}{\\partial y}] $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The whole basis of toolkits for Neural Networks is this simple Module API consisting of methods\n",
    "- `forward`\n",
    "- `backward`\n",
    "\n",
    "Knowing this: you can implement *your own* operations if you ever find that necessary.\n",
    "\n",
    "That is how more complex layers are implemented (e.g., Convolution).\n",
    "\n",
    "Hopefully this de-mystified the notion that Neural Network toolkits are complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
