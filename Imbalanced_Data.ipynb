{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\o}{\\mathbf{o}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced datasets\n",
    "\n",
    "What happens when our *training examples* are imbalanced\n",
    "- some examples over-represented\n",
    "- other examples under-represented\n",
    "\n",
    "We already briefly covered this in [Loss Analysis](Training_Loss.ipynb#Conditional-loss)\n",
    "- Our motivation there was focussing on examples where errors occur\n",
    "- Here our motivation is when the examples naturally partition into *imbalanced* subsets\n",
    "\n",
    "\n",
    "We revisit this in the case of a Binary Classification task, where one class dominates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Training loss is usually given unconditionally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "â€¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can also parititon the examples and examine the loss in each paritition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.jpg\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we paritition the training examples into those whose class is Positive and those whose class is Negative:\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\langle \\X, \\y \\rangle & = & [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ] \\\\\n",
    " & = & [ \\x^\\ip, \\text{Positive} | 1 \\le i \\le m' ] \\; \\cup \\; [ \\x^\\ip, \\text{Negative} | 1 \\le i \\le m'' ] \\\\ \n",
    " & = & \\langle \\X_{(\\text{Positive})}, \\y_{(\\text{Positive})} \\rangle \\; \\cup \\; \\langle \\X_{(\\text{Negative})}, \\y_{(\\text{Negative})} \\rangle\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can partition the training loss\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "That is, the Average loss is the weighted (with weigths $\\frac{m'}{m}, \\frac{m''}{m}$) conditional losses\n",
    "- ${ 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta$\n",
    "-  ${ 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we've observed before\n",
    "- As long as the majoirty class dominates in count (e.g., $m' \\gg m''$)\n",
    "- It is possible for Average Loss to be low\n",
    "- Even if Conditional Loss for the minority class is high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This means that training is less likely to generalize well out of sample to the minority examples.\n",
    "\n",
    "When the set of training examples $\\langle \\X, \\y \\rangle$ is such that\n",
    "- $\\y$ comes from set of categories $C$\n",
    "- Where the distribution of $c \\in C$ is *not* uniform\n",
    "\n",
    "the dataset is called *imbalanced*.\n",
    "\n",
    "This means that training is may be biased to not do as well on examples from under-represented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the Titanic survival:\n",
    "- only 38% of the passengers survived, so the dataset is highly imbalanced\n",
    "- a naive model that *always* predicted \"Not survived\" will\n",
    "    - have 62% accuracy\n",
    "        - be correct 100% of the time for 62% of the sample (those that didn't survive)\n",
    "        - be incorrect 100% of the time for 38% of the sample (those that did survive)\n",
    "\n",
    "The question is whether your use case requiress high accuracy in *all* classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approaches to imbalanced training data\n",
    "\n",
    "There are a number of approaches to avoid a potential bias caused by imbalanced data.\n",
    "\n",
    "[There is even a website](https://imbalanced-learn.readthedocs.io/en/stable/user_guide.html) with software approaches to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Loss\n",
    "- Use conditional metrics rather than unconditional metrics\n",
    "    - Metric less influenced by size\n",
    "    - Combination of Precision and Recall\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose a model that is not sensitive to imbalance\n",
    "- Decision Trees\n",
    "    - branching structure can handle imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss sensitive training\n",
    "\n",
    "Modify the Loss function to weight conditional probabilities\n",
    "\n",
    "Rather than\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta \\\\\n",
    "& = & \\frac{m'}{m} { 1\\over{m'} } \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\;  \\frac{m''}{m} { 1\\over{m''} } \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "adjust weights\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\loss_\\Theta  & = & \n",
    "C_{\\text{Positive}} * \\sum_{i' \\in \\X_{(\\text{Positive})}} \\loss^\\ip_\\Theta \\; + \\; C_{\\text{Negtive}} * \\sum_{i'' \\in \\X_{(\\text{Negative})}} \\loss^\\ip_\\Theta\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Equally weighted across classes: $C_\\text{Positive} = C_\\text{Negative}$\n",
    "- Relative importance\n",
    "    - An error in one class may be more important than an error in the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- `sklearn` inverse frequency weights\n",
    "    - user-defined weights (`sklearn` optional `class_weights` argument to some classification models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resampling\n",
    "\n",
    "Re-sampling is a process of constructing a new set of training examples by\n",
    "drawing samples from the original.\n",
    "\n",
    "The sampling doesn't have to be uniform and may be used such that the resampled examples are more balanced\n",
    "- We can oversample (draw with higher probability) the Minority class\n",
    "- We can undersample (draw with lower probability) the Majority class\n",
    "\n",
    "In the limit, weighted sampling is almost equivalent to using the original examples, but\n",
    "weighting the loss term for each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>Imbalanced data: Oversample the minority</center>\n",
    "    </tr>\n",
    "<img src=images/Oversample.jpg>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>Imbalanced data: Undersample the majority</center>\n",
    "    </tr>\n",
    "<img src=images/Undersample.jpg>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Caveat:**  No \"cheating\" in cross validation, again\n",
    "\n",
    "Does it make a difference if\n",
    "- We resample before cross validation ?\n",
    "- We resample *during* cross validation ?\n",
    "\n",
    "Yes !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- By resampling before cross validation\n",
    "    - the fold that is out-of-sample comes from the more-balanced resampled distribution\n",
    "- By resampling during cross validation\n",
    "    - Create the folds, including the out of sample fold\n",
    "    - Resample from the remaining folds to get a more balanced resampled distribution for this iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Synthetic examples\n",
    "\n",
    "Oversampling merely duplicates existing examples from the Minority class.\n",
    "\n",
    "There are techniques to generate *synthetic* examples\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A rough description of creating a synthetic example\n",
    "- Choose an example; find \"close\" neighboring examples of the same class (Minority)\n",
    "    - Using a metric of distance between examples\n",
    "- Create an example that blends/interpolates features from the neighbors into a new example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imbalanced example\n",
    "Here's an unbalanced dataset with 2 classes, and the associated predictions of some model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_true = np.array([0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([0, 1, 0, 0, 0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And the regular and class balanced accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acccuracy=0.667\n",
      "Class balanced Acccuracy=0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Acccuracy={a:3.3f}\".format(a=accuracy_score(y_true,  y_pred)))\n",
    "print(\"Class balanced Acccuracy={a:3.3f}\".format(a=balanced_accuracy_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's the math behind the two accuracy computations\n",
    "- \"Regular accuracy\": per class conditional accuracy, weight by class fraction as percent of  total\n",
    "- \"Balanced accuracy\": simple average of per class conditional accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy conditional on class=0 (66.67% of examples) = 0.750\n",
      "Accuracy conditional on class=1 (33.33% of examples) = 0.500\n",
      "\n",
      "\n",
      "Computed Accuracy=0.667 ( 66.67% * 0.750 + 33.33% * 0.500 )\n",
      "Computed Balanced Accuracy=0.625 ( average( 0.75, 0.5) )\n"
     ]
    }
   ],
   "source": [
    "# Enumerate the classes\n",
    "classes = [0,1]\n",
    "\n",
    "accs, weights = [], []\n",
    "\n",
    "# Compute per class accuracy and fraction\n",
    "for c in classes:\n",
    "    # Filter examples and predictions, conditional on class == c\n",
    "    cond = y_true == c\n",
    "    y_true_cond, y_pred_cond = y_true[cond],  y_pred[cond]\n",
    "    \n",
    "    # Compute fraction of total examples in class c\n",
    "    fraction = y_true_cond.shape[0]/y_true.shape[0]\n",
    "    \n",
    "    # Compute accuracy on this single class\n",
    "    acc_cond = accuracy_score(y_true_cond, y_pred_cond)\n",
    "    \n",
    "    print(\"Accuracy conditional on class={c:d} ({p:.2%} of examples) = {a:3.3f}\".format(c=c, \n",
    "                                                                                        p=fraction,\n",
    "                                                                                        a=acc_cond\n",
    "                                                                                     )\n",
    "         )\n",
    "    \n",
    "    accs.append(acc_cond)\n",
    "    weights.append(fraction)\n",
    "\n",
    "# Manual computation of accuracy, to show the math\n",
    "acc = np.dot( np.array(accs), np.array(weights) )\n",
    "acc_bal = np.average( np.array(accs) )\n",
    "\n",
    "eqn_elts = [ \"{p:.2%} * {a:3.3f}\".format(p=fraction, a=acc_cond) for fraction, acc_cond in zip(weights, accs) ]\n",
    "eqn_bal = \" + \".join( eqn_elts )\n",
    "\n",
    "eqn = \"average( {elts:s})\".format(elts=\", \".join([ str(a) for a in accs ]))\n",
    "print(\"\\n\")\n",
    "print(\"Computed Accuracy={a:3.3f} ( {e:s} )\".format(a=acc, e=eqn_bal))\n",
    "print(\"Computed Balanced Accuracy={a:3.3f} ( {e:s} )\".format(a=acc_bal, e=eqn) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
