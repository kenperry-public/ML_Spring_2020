{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " no stored variable _latex_std_\n"
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os \n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import unsupervised_helper\n",
    "%aimport unsupervised_helper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In *supervised learning*\n",
    "- we are given training examples  $\\langle \\X, \\y \\rangle$\n",
    "- we find a relationship between features $\\X$ and targets/labels $\\y$\n",
    "\n",
    "In *unsupervised learning*\n",
    "- we are given training examples $\\X$ \n",
    "- we find a relationship among the *features* $\\X$\n",
    "    - $\\y$ is not usually given (although we may refer to it for informational purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By finding groups of related features we may be able to demonstrate\n",
    "- Dimensionality reduction: reducing from $n$ original features to $n' \\le n$ *synthetic features*\n",
    "- Clustering of similar examples\n",
    "- Cleaning up noisy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "Why is the relationship among features interesting ?\n",
    "- Features may be interdependent (redundant)\n",
    "- Consider the MNIST digits\n",
    "    - Pairs of features in the 4 corners are highly correlated (e.g., mostly same color)\n",
    "    - Pairs of features in a vertical line (associated witht the digit \"1\") are somewhat correlated\n",
    "        - due to their coocurrence in 10% of the examples corresponding to \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because of the high pair-wise correlation, there may be a *more compact* way of representing the information\n",
    "- A new \"synthetic\" feature representing the presence of a \"concept\"\n",
    "    - \"Rectangle of same pixels\"\n",
    "    - \"Vertcal line of pixels\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's illustrate with a reduced dimension representation of the MNIST digits\n",
    "- Original feature vector length $n=784$\n",
    "- Reduced feature vector length $n' = 150$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We\n",
    "- Take the 784 *original* features\n",
    "- Create 150 *synthetic* features\n",
    "\n",
    "To demonstrate how little information is lost\n",
    "- We map *backwards* from the reduced $n'$ dimension representation back to the $n$ dimension represnetation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>PCA: reconstructed MNIST digits (95% variance)</center>\n",
    "    </tr>\n",
    "<img src=images/mnist_pca_95pct.jpg>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reconstructed $n=784$ feature digits are a little blurry but still recognizable.\n",
    "\n",
    "So 80% of the original $n=784$ features convey little information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction: examples\n",
    "\n",
    "**Color 3D movie to Black/white still image**\n",
    "- Lose Depth\n",
    "- Lose color of eyes/hair/clothing\n",
    "- Lose motion\n",
    "    - but pose may be informative\n",
    "\n",
    "For the purpose of recognizing a person, little information is lost\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Equity time series**\n",
    "\n",
    "Consider examples with $n=500$ features\n",
    "- $\\x^\\ip_j$ is the daily return of stock number $j$ on day $i$\n",
    "- Feature $\\x_j = [ \\x^\\ip_j | 1 \\le i \\le m ]$ (returns of equity $j$)\n",
    "- Highly correlated with most other features $\\x_{j'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way to interpret the high mutual correlation among equity returns\n",
    "- There is a *common influence* affecting all equities\n",
    "- e.g., An equity index reflecting the broad market\n",
    "- Pair-wise correlation of features arises through influence of the shared index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\x_1 = \\beta_1 * \\tilde\\x_\\text{index} + \\epsilon_1 \\\\\n",
    "\\x_2 = \\beta_2 * \\tilde\\x_\\text{index} + \\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\x_{500} = \\beta_{500} * \\tilde\\x_\\text{index} + \\epsilon_{500} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If each $\\epsilon_j$ is small (i.e., $\\tilde\\x_\\text{index}$ is a close approximation of $\\x_j$)\n",
    "- Then a single feature $\\x_\\text{index}$\n",
    "- Is an effection way of summarizing $\\x$, which has 500 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "Are the $m$ examples in the training set \n",
    "- Uniformly distributed across the $n$ dimensional space ?\n",
    "- Do they form *clusters* of examples with similar feature vectors ?\n",
    "\n",
    "Unfortunately: it's hard to visualize $n$ dimensions when $n$ is large.\n",
    "- By reducing the number of dimensions\n",
    "- We may be able to visualize related examples\n",
    "- In such a way that the reduced dimension examples don't lose too much information\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's illustrate with a limited subset of the smaller $(8 \\times 8)$ digits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>8 x 8 digits, subset</center>\n",
    "    </tr>\n",
    "<img src=images/digits_subset.jpg>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It would be difficult to visualize an example in $n=64$ dimensional space.\n",
    "\n",
    "By transforming example to a smaller number ($n'=2$ of synthetic features we *can* visualize\n",
    "- Each example is a point in two dimensional space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>8 x 8 digits, subset clusters</center>\n",
    "    </tr>\n",
    "<img src=images/digits_subset_cluster.jpg\">\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can see that our $m \\approx 700$ examples form 4 distinct clusters.\n",
    "- The clusters were formed\n",
    "    - Based solely on features\n",
    " \n",
    "It turns out that the clusters correspond to examples mostly representing a single digit.\n",
    "- The clusters organized themselves based on similarity of features\n",
    "- This is unsupervised ! No targets were used in forming the clusters!\n",
    "- We use the hidden target merely to color the point, not to form the clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This hints that dimensionality reduction may be useful for *supervised* learning as well\n",
    "- Use commonality of features to reduce dimension\n",
    "- Reduced dimensions more independent\n",
    "    - Better mathematically properies (reduced collinearity)\n",
    "    - More interpretable\n",
    "- Under assumption that\n",
    "    - Examples with similar features (i.e., in same cluster) have similar targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Noise reduction \n",
    "\n",
    "Consider the MNIST example, where we reduced $n$ by 80% without losing visual information.\n",
    "\n",
    "This might suggest that the 80% of the features dropped\n",
    "- Were signficant, but less important (dimension reduction)\n",
    "- OR that the dropped features were unimportant (noise)\n",
    "\n",
    "In the latter case, dropping features actually improves data quality by eliminating irrelevant feature.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix factorization\n",
    "\n",
    "We will learn how to find the \"most important\" features by\n",
    "factoring the example matrix $\\X$.\n",
    "\n",
    "The main tool we will introduce is called *Principal Components Analysis (PCA)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
