{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prepare data: transformations\n",
    "\n",
    "Transforming data (Recipe C.3) may be **the most important** step of the multi-step Recipe !\n",
    "\n",
    "It is often the case that the \"raw\" features given to us don't suffice \n",
    "- we may need to create \"synthetic\" features.  \n",
    "- This is called **feature engineering**.\n",
    "\n",
    "In the \"curvy\" data case, adding the squared feature was key to a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Feature engineering, or transformations\n",
    "- takes an example: vector $\\x^\\ip$ with $n$ features\n",
    "- produces a new vector $\\tilde\\x^\\ip$, with $n'$ features\n",
    "\n",
    "We ultimately fit the model with the transformed *training* examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Feature Engineering</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Feature_engineering.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above diagram shows multiple tranformations\n",
    "- organized as a sequence (sometimes called a *pipeline*) of independent transformations $T_1, T_2, \\ldots, T_t$\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\tilde{\\x}_{(1)} = T_1( \\x ) \\\\\n",
    "\\tilde{\\x}_{(2)} = T_1( \\tilde{\\x}_{(1)} ) \\\\\n",
    "\\vdots \\\\\n",
    "\\tilde{\\x}_{(\\ll+1)} = T_1( \\tilde{\\x}_{(\\ll)} ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We write the final transformed $\\tilde{\\x}$ as a function $T$ that is the composition\n",
    "of each transformation function\n",
    "$$\n",
    "\\tilde{x} = T(\\x) = T_t( \\; T_{t-1}( \\ldots T_1(\\x) \\ldots ) \\; )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The length of the final transformed vector $\\tilde{\\x}$\n",
    "may differ from the $n$, the length of the input $\\x$\n",
    "- may add features\n",
    "- may drop features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model $\\h(\\x; \\Theta)$ now takes $\\tilde{x}$ as input\n",
    "- The model is *fit* to *transformed* training examples\n",
    "- Prediction must be performed on *transformed* test examples !\n",
    "    - Examples must be treated consistently (e.g., have the same shape)\n",
    "    - Regardless of whether the example is from training, validation or test\n",
    "    \n",
    "It is thus very important to apply the transformation pipeline to every example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples\n",
    "\n",
    "The first transformation we encountered added a feature ($\\x^2$ term) that improved prediction.\n",
    "\n",
    "Some transformations alter existing features rather than adding new ones.\n",
    "\n",
    "Transformations in detail will be the subject of a separate lecture but let's cover the basics.\n",
    "\n",
    "Let's consider a second reason for transformation: filling in (imputing) missing data for a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$#$ | $x_1$ | $x_2$\n",
    " -- | ----- | ----\n",
    "1   | 1.0   | 10\n",
    "2   | 2.0   | 20\n",
    "$\\vdots$ | $\\vdots$ | $\\vdots$ \n",
    "i   | 2.0   |  NaN\n",
    "$\\vdots$ | $\\vdots$ | $\\vdots$ \n",
    "m   | $\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above: feature $\\x_2$ is missing a value in example $i$: \\$x^\\ip_2 = \\text{NaN} $\n",
    "\n",
    "We will spend more time later discussing the various ways to deal with missing data imputation.\n",
    "\n",
    "For now: let's adopt the common strategy of replacing it with the median of the defined values:\n",
    "\n",
    "$$\\text{median}(\\x_2) = \\text{median}( \\{ \\x^\\ip_2 | 1 \\le i \\le m, \\x^\\ip_2 \\ne \\text{NaN} \\} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This imputation is a kind of data transformation: replacing an undefined value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#  \"Fitting\" transformations\n",
    "\n",
    "Transformations often have their own parameters $\\Theta_\\text{transform}$ that is separate from\n",
    "the $\\Theta$ parameters of the model.\n",
    "\n",
    "In the case of imputation: \n",
    " the mean/median of a feature $j$ for a missing value.\n",
    "\n",
    "In that case: $\\Theta_\\text{transform}$ must contain $ \\text{median}(\\x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The process of Transformations is similar to fitting a model and predicting.\n",
    "\n",
    "The parameters in $\\Theta_\\text{transform}$ \n",
    "- are \"fit\" by examining all training data $\\X$\n",
    "- once fit, we can transform (\"predict\") *any* example (whether it be training/validation or test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applying transformations consistently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that the transformation of an example depends on $\\Theta_\\text{transform}$, which is fit\n",
    "*only on the training data*.\n",
    "\n",
    "When transforming any example (including one *not* in $\\X$) one uses $\\Theta_\\text{transform}$ from\n",
    "the transformation fitting\n",
    "- You **do not** recalculate $\\Theta_\\text{transform}$ on test examples !\n",
    "    - Just imagine that you are given each test example in isolation-- there is no summary statistic to compute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Feature engineering: fit, then transform</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Feature_engineering_fit.jpg\" width=1000</td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To re-iterate:\n",
    "- **No** fitting is applied to test examples only train !\n",
    "- The $\\Theta_\\text{transform}$ obtained from training data is used in transforming *test* as well as *train* examples\n",
    "\n",
    "There are several reasons not to re-fit on test examples\n",
    "- it would be a kind of \"cheating\" to see all test examples (required to fit)\n",
    "- you should assume that you only encounter one test example at a time, not as a group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transformations are applied to both training and test examples\n",
    "- training examples so that the model may be fit\n",
    "- test examples in order to be able to predict\n",
    "    - to the extent that transformations added features (e.g., $\\x^2$) or changed features (imputation)\n",
    "    - the test examples *must be transformed* the same way as training\n",
    "        - otherwise they won't be similar to training examples, violating the fundamental assumption of ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using pipelines to avoid cheating in cross validation\n",
    "\n",
    "We know that looking at test examples while fitting a model is \"cheating\".\n",
    "\n",
    "That's one of the reasons that $\\Theta_\\text{transform}$ is fit only using training examples.\n",
    "\n",
    "There is a subtle (but common and often overlooked) form of cheating that occurs when using\n",
    "cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$k$-fold cross-validation:\n",
    "- Allows us to use part of the training examples as \"out of sample\" for computing the Performance Metric\n",
    "    - We can reuse these out of sample examples multiple times -- we couldn't do this with the test examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The process of cross-validation\n",
    "- Divides the training examples into $k$ \"folds\"\n",
    "- A model is fit $k$ times\n",
    "    - by selecting one of the folds to serve the role of \"out of sample\" (on which the Performance Metric is evaluated)\n",
    "    - using the other $(k-1)$ folds as the training data\n",
    "- This gives us $k$ Performance Metrics, from which we can see the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    " <tr>\n",
    "        <th><center>Cross Validation/Test split</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Cross_validation.jpg\"</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the difference between\n",
    "- Transforming *all* the training examples *before* applying cross-validation\n",
    "- For each iteration of Cross-Validation: transforming only the examples from the $(k-1)$ folds used\n",
    "\n",
    "In the first case, $\\Theta_\\text{transform}$ depends on *all* $k$ folds\n",
    "- Even though during each iteration of Cross Validation, one fold should be treated as out of sample\n",
    "- In the first case, each iteration of Cross Validation could be influenced by out of sample examples\n",
    "\n",
    "Performing the transformation within each iteration of Cross Validation avoids peeking at out of sample examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Perhaps the reason that this subtle cheating is overlooked is because\n",
    "it might make the use of cross validation burdensome.\n",
    "\n",
    "`sklearn` has been engineered to make it easy to perform transformations in the proper manner.\n",
    "\n",
    "We will see this in action within the notebook for Classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
