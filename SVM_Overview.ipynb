{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "svm_ch = svm_helper.Charts_Helper(save_dir=\"/tmp\", visible=False)\n",
    "\n",
    "import class_helper\n",
    "%aimport class_helper\n",
    "\n",
    "clh = class_helper.Classification_Helper()\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Boundary chart\n",
      "Create Sens chart\n",
      "Create margin chart\n",
      "Done\n",
      "{'boundary': '/tmp/svm_sep_boundary.jpg', 'sensitivity': '/tmp/svm_sens.jpg', 'margin': '/tmp/svm_margin.jpg'}\n"
     ]
    }
   ],
   "source": [
    "# Create files containing charts\n",
    "create = False\n",
    "\n",
    "if create:\n",
    "    file_map = svm_ch.create_charts()\n",
    "    print(file_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Classifier (SVC)\n",
    "We introduce another model for the Classification task: the Support Vector Classifier (SVC).\n",
    "\n",
    "We motivate this classifer by some examples.\n",
    "\n",
    "Consider the following linearly separable dataset and two separating lines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Linear separating boundary</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/svm_sep_boundary.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Is one separating line \"better\" than the other ?\n",
    "\n",
    "Intuitively,the first line feels more fragile\n",
    "- The distance between the line and the closest example is smaller in the first plot than the second\n",
    "- If the red point located exactly on the line moved a small amount, we would misclassify it\n",
    "- Not as likely to generalize out of sample as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Something else to consider:\n",
    "- How sensitive is our classifier to additional examples *that are far from the original boundary**\n",
    "\n",
    "Consider the two rows in the plot below\n",
    "- The second plot in each row adds a cluster of examples in the bottom right corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sensitivity to far-from-boundary examples</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/svm_sens.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you can see\n",
    "- Logistic Regression is sensitive\n",
    "- SVC is not\n",
    "\n",
    "Why should points far away from the original separating boundary affect the fit ?\n",
    "\n",
    "The answer, of course, is the Loss function.\n",
    "\n",
    "We will see that the SVC uses a much different loss that has several advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SVC introduces two additional separating lines that are parallel to the separating\n",
    "boundary\n",
    "- at a distance $\\margin$ (measured by length of a line orthogonal to the boundary) on either side of the boundary\n",
    "- $\\margin$ is called the *margin*\n",
    "- these two lines define a \"buffer\" of width twice the margin\n",
    "\n",
    "We draw two plots with diferent size margins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Margin</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/svm_margin.jpg\"></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The concept of margin helps to address the two issues raised above.\n",
    "\n",
    "- Given a choice of two separating boundaries (each with perfect separation)\n",
    "    - the one with the larger margin is \"better\" \n",
    "- An example that is correctly classified and lies *outside* the buffer should not affect the Loss Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the above examples\n",
    "- It may not be possible to have both a large margin and a boundary that separates classes perfectly\n",
    "    - the left plot, for example\n",
    "\n",
    "Requiring perfect separation and no examples in the buffer is called *Hard Margin* classification.\n",
    "- allowing either condition to be false is called *Soft Margin* classification\n",
    "\n",
    "A *Soft Margin* classifier allows violations but imposes a *penalty* (by increasing the Loss).\n",
    "\n",
    "An SVC classifier is a Soft Margin classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The main difference between an SVC and Logistic Regression classifier are in their loss functions.\n",
    "\n",
    "The loss function for an SVC contrasts with Cross Entropy (the loss of Logistic Regression) in that\n",
    "- there are examples with **zero** loss\n",
    "    - those that are correctly classified and outside the buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will dig into the loss function for the SVC shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "As we have seen before many datasets, in raw form, are not linearly separable.\n",
    "\n",
    "Transformations of the raw data must be applied to induce Linear Separability.\n",
    "\n",
    "A *Support Vector Machine* is the process of applying transformation functions and then\n",
    " using an SVC.\n",
    " \n",
    "There is a special subclass of transformation functions called **kernel functions** that\n",
    "- uses a clever mathematical trick \n",
    "- to achieve the effect of applying an expensive transformation without actually creating transformed data !\n",
    "\n",
    "The SVM helps to automate **does it jointly solve for hyperparameters of the transformation ??** the transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Key concepts\n",
    "\n",
    "Just like Logistic Regression, the SVC will:\n",
    "- Use the features $\\x^\\ip$ to compute a \"score\" $\\hat{s}^\\ip$\n",
    "- Compare the predicted score to a threshold\n",
    "- Predict Positive if the score exceeds the threshold; Negative otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The score is linear in the features\n",
    "$$\\begin{array}[lll]\\\\\n",
    "s(\\x) = \\Theta^T \\x & \\text{score} \\\\\n",
    "s(\\x) = 0 & \\text{equation of separating boundary} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "\\hat{y}^\\ip = \n",
    "\\left\\{\n",
    "    {\n",
    "    \\begin{array}{lll}\n",
    "     \\text{Negative} & \\textrm{if } \\hat{s}^\\ip   < 0  &  \\\\\n",
    "      \\text{Positive}& \\textrm{if } \\hat{s}^\\ip \\ge 0  \n",
    "    \\end{array}\n",
    "    }\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SVC and SVM incorporate several \"tricks\"\n",
    "- A clever Loss Function (Hinge Loss)\n",
    "- Large Margin Classification\n",
    "- Kernel Transformations\n",
    "\n",
    "The combined effect can be overwhelming and makes the SVC/SVM seem complex\n",
    "- and it *does* involve a bit of math\n",
    "\n",
    "We will try to reduce the complexity by tackling each trick separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
