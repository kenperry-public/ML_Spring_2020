{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import training_models_helper as tmh\n",
    "%aimport training_models_helper\n",
    "\n",
    "tm = tmh.TrainingModelsHelper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()\n",
    "\n",
    "kn = tmh.KNN_Helper()\n",
    "\n",
    "import transform_helper\n",
    "%aimport transform_helper\n",
    "\n",
    "th = transform_helper.Transformation_Helper()\n",
    "\n",
    "iph = transform_helper.InfluentialPoints_Helper()\n",
    "\n",
    "import svm_helper\n",
    "%aimport svm_helper\n",
    "svmh = svm_helper.SVM_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Understanding the Loss function\n",
    "\n",
    "In performing Error Analysis (post-training) out of sample, we *identified* examples where our model failed to generalize.\n",
    "\n",
    "What can we do to make the model better ?  How can we influence the models' choice of $\\Theta$ to lead\n",
    "to a better fit ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the event that the Performance Metric (evaluated out of sample) and the Loss Function (evaluated in sample)\n",
    "differ\n",
    "- We must see how we can influence the Loss function\n",
    "- In the hope that better in sample performance leads to better out of sample performance\n",
    "\n",
    "Now is a good time to recall the distinction between Accuracy (Performance Metric) and Cross Entropy (Loss function) for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall the mapping of probability to prediction\n",
    "\n",
    "$$\n",
    "\\hat{y}^\\ip = \n",
    "\\left\\{\n",
    "    {\n",
    "    \\begin{array}{lll}\n",
    "     \\text{Negative} & \\textrm{if } \\hat{p}^\\ip   < 0.5  \\\\\n",
    "     \\text{Positive}& \\textrm{if } \\hat{p}^\\ip \\ge 0.5 \n",
    "    \\end{array}\n",
    "    }\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where, for Logistic Regression,  probability $\\hat{p}^\\ip$ is a function of $\\x^\\ip$ and parameters $\\Theta$.\n",
    "$$\n",
    "\\hat{p}^\\ip = \\sigma( \\Theta^T \\cdot \\x^\\ip )\n",
    "$$\n",
    "- Accuracy (for example $i$) won't *necessarily* vary with $\\Theta$ unless $\\hat{p}^\\ip$ crosses the threshold of $0.5$\n",
    "- But $\\hat{p}^\\ip$ will vary with $\\Theta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, a $\\Theta'$ which pushes $\\hat{p}^\\ip$ closer to the correct probability (0 or 1)\n",
    "may be preferred to a $\\Theta$ that leaves $\\hat{p}^\\ip$ farther away.\n",
    "\n",
    "In this section\n",
    "- We will be using *training examples* (in-sample) rather than out of sample data.\n",
    "- In an attempt to reduce the Loss function\n",
    "- Under the assumption that better in sample performance will lead to better out of sample performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that \n",
    "- the model is a function of parameters $\\Theta$\n",
    "- $\\Theta$ is found by minimizing Average Loss $\\loss_\\Theta$\n",
    "- The Average Loss is the average of the per-examples losses $\\loss_\\Theta^\\ip, i=1, \\ldots, m$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_training_1.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_training_2.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_training.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training Example</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n",
    "•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conditional loss\n",
    "\n",
    "The key to improving the model is understanding who each per example loss contributes\n",
    "to the optimizer choosing $\\Theta$.\n",
    "\n",
    "One way to try to improve the model is to look at the per-example losses in Training\n",
    "- similar to the way we looked at Errors out of sample\n",
    "- colors represented groups similar examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Loss analysis: conditional loss</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Intro_error_analysis_1.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What can we do to reduce loss ?\n",
    "\n",
    "Understanding the per example loss can help you \"push\" the optimizer toward find a \"better\" $\\Theta$.\n",
    "\n",
    "We will outline some simple strategies via examples that identify a probelm and propose a solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Increase number of \"problem\" training example\n",
    "\n",
    "In our MNNIST digit classification error analysis, we identified a certain sub-class of the digit \"8\" that was mis-classified\n",
    "- at least one of the \"holes\" in the 8 was very small\n",
    "- the digit was slanted in \"opposite\" direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$\n",
    "\n",
    "So problem example $i$'s contribution to Average Loss is ${ 1\\over{m} }\\loss^\\ip_\\Theta$.\n",
    "\n",
    "If the number of problem training examples of a particular type is small, the sum  of the per example losses\n",
    "due to the problem examples may not have enough of an impact on $\\loss$ to affect the solution $\\Theta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One strategy for pushing the model to better fit the problem examples is to increase their number !\n",
    "- so total weight of this particular class of problems has greater impact on $\\loss$\n",
    "\n",
    "If you can find (or synthesize) similar types of problem examples, adding them to the training set\n",
    "forces the optimizer to better accomodate these examples.\n",
    "\n",
    "We will introduce *Data Augmentation* is a later module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decrease the influence of a \"problem\" example\n",
    "\n",
    "Sometimes the problem is not having too few \"problem\" examples, but is having a few \"problems\" that\n",
    "are so off-scale that they unduly influence $\\Theta$.\n",
    "\n",
    "- That is: $\\loss_\\Theta^\\ip$ is so large that it dominates $\\loss$ and forces $\\Theta$ to accomodate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Influential points\n",
    "\n",
    "Some models may be quite sensitive to just a few observations.\n",
    "\n",
    "This is particularly true for Linear Regression.\n",
    "\n",
    "Our discussion is somewhat specialized to Linear Regression but you may come to see a similar\n",
    "phenomenon in other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Loosely speaking, an observation is **influential** if \n",
    "- the parameter estimate $\\Theta$ changes greatly depending on whether the observation is included/excluded\n",
    "\n",
    "Feature values on the extreme ends of the range have greater potential\n",
    "for being influential.\n",
    "\n",
    "This is one argument for constraining the range of the feature (MinMax, Standardization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The **leverage** of an observation is related to the value of a feature in relation to the mean (across observations) of the feature\n",
    "- extreme values of the feature have higher leverage\n",
    "\n",
    "It is not always the case, but high leverage sometimes makes the point influential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Influence from leverage and distance](http://onlinestatbook.com/2/regression/influential.html)\n",
    ">An observation's influence is a function of two factors: (1) how much the observation's value on the predictor variable differs from the mean of the predictor variable and (2) the difference between the predicted score for the observation and its actual score. The former factor is called the observation's leverage. The latter factor is called the observation's distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Calculation of Leverage (h) of example $i$, feature $j$\n",
    "\n",
    "[formula](https://learnche.org/pid/least-squares-modelling/outliers-discrepancy-leverage-and-influence-of-the-observations#leverage)\n",
    "\n",
    "$$ \n",
    "\\begin{array}{lll}\n",
    "h^\\ip_j & = & { 1 \\over n }+ \\frac{ (\\x^\\ip_j - \\bar{\\x_j})^2}{ \\sum_i { (\\x^\\ip_j - \\bar{\\x_j})^2} } \\\\\n",
    "    & = & \\frac{ 1 + \\left( \\frac{\\x^\\ip_j - \\bar{\\x_j}}{\\sigma_{\\x_j} } \\right) ^2}{n}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "You can see that the leverage of $\\x^\\ip_j$ depends on the (standardized) distance of $x^\\ip_j$ from the mean (over all $i$) of $\\x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's an interactive tool to get a feel for influential points.\n",
    "\n",
    "It allows you to change the value of a single data point and see how the Linear Regression is affected.\n",
    "\n",
    "Observe how the slope changes (displayed in the title)\n",
    "- The `x_l` slider chooses the index of the data point to change\n",
    "- The `y_l` slider chooses how much the data point changes\n",
    " - i.e., will change $\\x^\\ip$ when `x_l = i`\n",
    "- 10 data points\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate some points\n",
    "(x_ip,y_ip) = iph.gen_data(10)\n",
    "\n",
    "# Fit a line to the points; get a function to update the fit and the plot\n",
    "fit_update = iph.plot_init()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2846d82e1204bb68d2643eb75d43607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='x_l', max=9), IntSlider(value=0, description='y_l', max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iph.plot_interact(fit_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Play around with the example\n",
    "- choose a point to move using the top slidier\n",
    "- choose how much to move the chosen point with the bottom slider\n",
    "- see the effect of the change on the Slope (in the title)\n",
    "\n",
    "Observe \n",
    "- changing a point in the middle has little effect on the slope\n",
    "- changing a point closer to either extreme can have a big effect on the slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This illustrates the effect of a single example on $\\Theta$\n",
    "\n",
    "Knowing how influential the point is on $\\Theta$ mave cause you to reduce its influence\n",
    "- drop the example\n",
    "    - possible error, outlier\n",
    "- clip the value (bound the range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
