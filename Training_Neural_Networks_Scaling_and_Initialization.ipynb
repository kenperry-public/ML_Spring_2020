{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proper scaling of inputs\n",
    "\n",
    "## Importance of zero centered inputs (for each layer)\n",
    "[Efficient Backprop paper, LeCunn98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "**Zero centered** means average (over the training set) value of each feature of examples is mean $0$.\n",
    "\n",
    "Gradient descent updates each element of a layer $\\ll$'s weights $\\W_\\llp$ by\n",
    "the per-example losses \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial W_\\llp} & = & \\frac{\\partial \\loss^\\ip}{\\partial \\y_\\llp^\\ip} \\frac{\\partial \\y_\\llp^\\ip}{\\partial \\W_\\llp} \n",
    "\\end{array}\n",
    "$$\n",
    "summed over examples $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Over-simplifying:\n",
    "- the local derivative is proportional to the input:\n",
    "$$\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_\\llp } = a'_\\llp \\y_{(\\ll-1)}^\\ip\n",
    "$$\n",
    "for FC $y_\\llp = a_\\llp ( \\y_{(\\ll-1)} \\W_\\llp )$.\n",
    "- thus the updates of $\\W_{\\llp,j}$ will be biased by $\\bar{\\y}_{(\\ll-1),j}$ = the average (over examples $i$) of $\\y_{(\\ll-1),j}^\\ip$\n",
    "- for $\\ll = 1$, this is the average of the input feature $\\x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular case that each feature $j$'s average $\\bar{\\x}_j$ has the same sign:\n",
    "- updates in all dimensions will have the same sign\n",
    "- this can result in an indirect \"zig-zag\" toward the optimum\n",
    "    - Exampe: two dimensions: \n",
    "        - We can navigate the loss surface north-east or south-west only ! \n",
    "        - To get to a point north-west from the current, we have to zig-zag.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that this is an issue for *all* layers, not just layer $\\ll =1$.\n",
    "\n",
    "- Also note: the problem is compounded by activations whose outputs are not zero-centered (e.g., ReLU, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importance of unit variance inputs (weight initialization)\n",
    "\n",
    "The same argument we made for zero-centering a feature can be extended to it's variance:\n",
    "- the variance of feature $j$ over all training examples $i$ is the varaince of $\\y_{(\\ll-1),j}$\n",
    "\n",
    "If the variance of features $j$ and $j'$ are different, their updates will happen at different rates.\n",
    "\n",
    "We will examine this in greater depth during our discussion of weight initialization.\n",
    "\n",
    "For now: it is desirable that the input to *each* layer have it's features somewhat normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Training is all about discovering good weights.\n",
    "\n",
    "As prosaic as it sounds: how do we *initialize* the weights before training ?\n",
    "Does it matter ?\n",
    "\n",
    "It turns out that the choice of initial weights does matter.\n",
    "\n",
    "Let's start with some *bad* choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad choices\n",
    "\n",
    "### Too big/small\n",
    "\n",
    "Layers usually consist of linear operations (e.g., matrix multiplication and addition of bias)\n",
    "followed by a non-linear activation.\n",
    "\n",
    "The range of many activation functions includes large regions where the derivatives are near zero,\n",
    "usually corresponding to very large/small activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Gradient Descent updates weights using the gradients.\n",
    "\n",
    "Obviously, if the gradients are all near-0, learning cannot occur.\n",
    "\n",
    "So one bad choice is any set of weights that tends to push activations to regions of the non-linear\n",
    "activation with zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identical weights\n",
    "\n",
    "Consider layer $\\ll$ with $n_\\ll$ units (neurons) implementing identical operations (e.g. FC + ReLu).\n",
    "\n",
    "Let  $\\W_{\\llp, k}$ denote the weights of unit $k$.\n",
    "\n",
    "Suppose we initialized the weights (and biases) of all units to the *same* vector.\n",
    "$$\n",
    "\\W_{\\llp, k} = \\w_\\llp, \\; 1 \\le k \\le n_\\ll\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two neuron $j, j'$ in the same layer $\\ll$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{\\llp, j}  & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\y_{\\llp, j'} & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Both neuron will compute the same activation\n",
    "- Both neurons will have the same gradient\n",
    "- Both neurons will have the same weight update\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, the weights in layer $i$ will start off identical and will remain identical due to identical updates!\n",
    "\n",
    "Neurons/units $j$ and $j'$ will never be able to differentiate and come to recognize *different* features.\n",
    "\n",
    "This negates the advantage of multiple units in a layer.\n",
    "\n",
    "Many approaches use some for of random initialization to break the symmetry we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot initialization\n",
    "\n",
    "We have previousy shown that each element $j$ of the first input layer ($\\x_{(0),j}$) should\n",
    "have unit variance across the training set.  \n",
    "\n",
    "This was meant to ensure that the first layer's weights\n",
    "updated at the same rate and that the activations of the first layer fell into regions of the activation\n",
    "function that had non-zero gradients.\n",
    "\n",
    "But this is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume for the moment that each element $j$ of the input vector $\\y_{(\\ll-1)}$ is mean $0$, unit variance\n",
    "and mutually independent.  \n",
    "\n",
    "So view each $\\y_{(\\ll-1),j}$ as an independent random variable with mean $0$\n",
    "and unit variance.  \n",
    "\n",
    "Furthermore, let's assume each element $\\W_{\\llp,j}$ is similarly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the dot product in layer $\\ll$ \n",
    "$$f_\\llp(\\y_{(\\ll-1)}, W_\\llp) = \\y_{(\\ll-1)} \\cdot W_\\llp$$\n",
    "\n",
    "Recall that layer $(\\ll-1)$ has $n_{(\\ll-1)}$ outputs.\n",
    "\n",
    "Thus, the dot product is the sum over $n_{(\\ll-1)}$ pair-wise products \n",
    "- $\\y_{(\\ll-1),j} * \\W_{\\llp,j}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *variance* of a product of random variables $X, Y$ \n",
    "[is](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)\n",
    "\n",
    "$$\n",
    "\\text{Var}(X * Y) = \\mathbb{E}(X)^2 \\text{Var}(Y) + \\mathbb{E}(Y)^2 \\text{Var}(X) + \\text{Var}(X)\\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\text{Var}(\\y_{(\\ll-1),j} * \\W_{\\llp,j}) & = & 0^2 * 1 + 0^2 * 1 + 1 * 1 \\\\\n",
    "& = & 1 & \\text{Since } \\y_{(\\ll-1),j} \\text{ and } \\W_{\\llp,j} \\text{are mean } 0 \\text{ variance } 1\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus \n",
    "- The variance of the dot product involving $n_{(\\ll-1)}$ pair-wise products\n",
    "- Is $n_{(\\ll-1)}$, not $1$ as desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can force the dot product to have unit variance\n",
    "- By scaling each $\\W_{\\llp,j}$ by \n",
    "$$\n",
    "\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$$\n",
    "\n",
    "This is the basis for *Glorot/Xavier Initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Sets the initial weights to a number drawn from a\n",
    "mean $0$, unit variance distribution (either normal or uniform)\n",
    "- Multiplied by $\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$.\n",
    "\n",
    "Note that we don't strictly need the requirement of *unit* variance \n",
    "- It suffices that the input and output variances are *equal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only partially solves the problem as it only ensures unit variance of the **input** to the activation function.\n",
    "\n",
    "The [original Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) justifies this\n",
    "- By assuming either a $\\tanh$ or sigmoid activation function\n",
    "- Which are approximately linear in the active region.\n",
    "- So the **output** of the activation function is equal to the input in this region\n",
    "- And is therefore unit variance as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus far, we have achieved unit variance during the forward pass.\n",
    "\n",
    "During back propagation\n",
    "- It can be  shown that the scaling factor\n",
    "- Depends on the number of outputs $n_\\llp$ of layer $\\ll$, rather than the number of inputs $n_{(\\ll-1)}$\n",
    "- Thus, the scaling factor needes to be $\\frac{1}{\\sqrt{n_\\llp}}$ rather than $\\frac{1}{\\sqrt{n_{\\ll-1}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Taking the average of the two scaling factors gives a final factor of\n",
    "$\\frac{1}{\\sqrt{ \\frac{ n_{\\ll-1} + n_\\ll}{2} } } = \\sqrt{\\frac{2}{n_{\\ll-1} + n_\\ll}}\n",
    "$\n",
    "\n",
    "which is what you often see in papers using this form of initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaiming/He initialization\n",
    "\n",
    "Glorot/Xavier initialization was tailored to two particular activation functions ($\\tanh$ or sigmoid).\n",
    "\n",
    "[Kaiming et al](https://arxiv.org/pdf/1502.01852.pdf) extended the results\n",
    "to the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU activation has two distinct regions: one linear (for inputs greater than 0) and one all zero.\n",
    "\n",
    "The linear region of the activation corresponds to the assumption of the Glorot method.\n",
    "\n",
    "So if inputs to the ReLU are equally distributed around 0, this is approximately the same\n",
    "as the Glorot method with half the number of inputs.\n",
    "- that is: half of the ReLU's will be in the active region and half will be in the inactive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kaiming scaling factor is thus:\n",
    "$$\n",
    "\\sqrt{\\frac{2}{n_{(\\ll-1)}} }\n",
    "$$\n",
    "in order to preserve unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training\n",
    "\n",
    "In the early days of Deep Learning\n",
    "- Before good weight initialization techniques were discovered\n",
    "- A technique called *Layer-wise pre-training* was very popular\n",
    "\n",
    "This technique is best discussed after we introduce Autoencoders but, for now:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we want to initialize the weights of layer $\\ll$\n",
    "- We *temporarilty* create a two layer sub-network consisting of layers $\\ll$ and a \"Decoder\" layer\n",
    "- Let $\\y_{(\\ll+1)}'$ denote the  output of this two layer network\n",
    "- This two layer network has input $\\y_{(\\ll-1)}$ and output $\\y_{(\\ll+1)}'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We train this two layer network with training examples\n",
    "\n",
    "$$ \\langle \\y_{(\\ll-1)}, \\y_{(\\ll-1)} \\rangle= [ \\y^\\ip_{(\\ll-1)}, \\y^\\ip_{(\\ll-1)} \\; | \\; 1 \\le i \\le m ]$$\n",
    "\n",
    "That is: we are asking the two layer network to implement the identity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To avoid making this trivial\n",
    "- The Decoder creates a \"bottle-neck\": a layer with a small number of units\n",
    "- The small number prevents the Decoder from memorizing the inputs\n",
    "- This is a form of *dimensionality reduction* (like Principal Components Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Like Principal Components Analysis\n",
    "- We are asking the two layer network\n",
    "- To discover a small number of synthetic features that summarize the diversity of $\\y_{(\\ll-1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The weights discovered by this procedure become the initial weights of layer $\\ll$\n",
    "- The Decoder is then dropped\n",
    "- And we proceed to find initial weights for layer $(\\ll+1)$.\n",
    "\n",
    "These weights *may or may not* be useful in predicting $\\hat{\\y} = \\y_{(L)}$\n",
    "- But they are probably better than *random* weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization\n",
    "\n",
    "- We addressed the importance of normalization of the inputs to layer $\\ll = 1$.\n",
    "- The same argument applies to *all* layers $\\ll > 0$\n",
    "\n",
    "We discuss some Normalization methods that attempt to keep the distribution of $\\y_{\\llp,j}$\n",
    "normalized through all layers $\\ll$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch normalization\n",
    "[Batch Normalization paper](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "The idea behind batch normalization:\n",
    "-  perform standardization  (mean $0$, standard deviation 1)\n",
    "at each layer, using the mean and standard deviation of each minibatch.\n",
    "\n",
    "- faciliates higher learning rate \n",
    "    - controlling the size of the derivative allows higher $\\alpha$ without increasing product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experimental results show that the technqiue:\n",
    "- facilitates the use of much higher learning rates, thus speeding training.  Accuracy is not lost.\n",
    "- facilitates the use of saturating activations functions (e.g., $\\tanh$ and sigmoid) which otherwise are subject to vanishing/exploding gradients.\n",
    "- acts as a regularizer; reduces the need for Dropout\n",
    "    - L2 regularization (weight decay) has *no* regularizing effect when used with Batch Normalization !\n",
    "        - [see](https://arxiv.org/abs/1706.05350)\n",
    "        - L2 regularization affects scale of weights, and thereby learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Details\n",
    "\n",
    "Consider a FC layer $\\ll$ with $n_\\ll$ outputs and a mini-batch of size $m_B$.\n",
    "\n",
    "Each of the $n_\\ll$ outputs is the result of\n",
    "- passing a linear combination of $\\y_{(\\ll -1)}$ (*activation inputs*)\n",
    "-  through an activation $a_{\\llp,j}$ (*activation outputs*)\n",
    "\n",
    "We could choose to standardize either the activation inputs or the activation outputs.\n",
    "\n",
    "This algorithm standardizes the **activation inputs**.\n",
    "\n",
    "Standardization is performed relative to the mean and standard deviation of each batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Summary for layer $\\ll$ with equation $\\y_\\llp = a_\\llp( \\W_\\llp \\y_{(\\ll-1)})$\n",
    "- each output feature $j$: $\\y_{\\llp,j} = a_{\\llp,j}( \\W_{\\llp,j} \\y_{(\\ll-1)})$\n",
    "\n",
    "- Denote the dot product for output feature $j$ by $\\x_{\\llp,j} = \\W_{\\llp,} \\y_{(\\ll-1)}$\n",
    "- We will replace $\\x_{\\llp,j}$ by a \"standardized\" $\\z_{\\llp,j}$ to be described\n",
    "\n",
    "Rather than carrying along supscript $j$\n",
    "we write all operations on  the collection $\\x_{\\llp,j}$ as a vector operation on $\\x_\\llp$ for ease of notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
    "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So\n",
    "- $\\mathbf{\\mu}_B, \\mathbf{\\sigma}_B$ are vectors (of length $n_\\ll$) of \n",
    "    - the element-wise means and standard deviations (computed across the batch of $m_B$ examples)\n",
    "- $\\mathbf{\\hat{x}^{(i)}}$ is standardized $\\mathbf{x}^{(i)}$ \n",
    "\n",
    "** Note the $\\epsilon$ in the denominator is their solely to prevent \"divide by 0\" errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is going on with $\\z^\\ip$ ?  \n",
    "\n",
    "Why are we constructing it with mean $\\beta$ and standard deviation $\\gamma$ ?\n",
    "\n",
    "$\\beta, \\gamma$ which are **learned** parameters.\n",
    "\n",
    "Why should $\\beta, \\gamma$ be learned ?\n",
    "\n",
    "At a minimum: it can't hurt:\n",
    "- it admits the possibility of the identity transformation\n",
    "    - which would be the simple standardization\n",
    "- but allows the unit to be non-linear when there is a benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, depending on the activation $a_{\\llp, j}$\n",
    "- $\\hat{\\x}_{\\llp,j}$ can wind up *within the active region* of the activation function\n",
    "\n",
    "This effectively makes our transformations linear, rather than non-linear, which are more powerful.\n",
    "\n",
    "By shifting the mean by $\\beta$ we gain the *option* to avoid this should it be beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final question is: what do we do at inference/test time, when all \"batches\" are of size 1 ?\n",
    "\n",
    "The answer is\n",
    "- compute a single $\\mathbf{\\mu}, \\mathbf{\\sigma}$ from the sequence of such values across all batches.\n",
    "- \"population\" statistics (over full training set\n",
    "- rather than \"sample\" statistics (from a single training batch).\n",
    "\n",
    "Typically a moving average is used.\n",
    "We refer readers to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create a new layer type $\\text{BN}$ to perform Batch Normalization to the inputs of any layer.\n",
    "\n",
    "Thus, it particpates in both the forward (i.e., normalization) and backward (gradient computation)\n",
    "steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unbelievably good initialization\n",
    "\n",
    "We have seen several methods that attempt to create \"good\" weights\n",
    "Glorot and Kaiming weight initialization \n",
    "- ensures \"good\" distribution of outputs of a layer, given a good distribution of inputs to the layer\n",
    "\n",
    "Normalization (e.g., Batch Normalization)\n",
    "- tries to ensure good distribution of inputs across all layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are some initialization methods that attempt to create weights that are so good,\n",
    "that Normalization during training is no longer necessary.\n",
    "\n",
    "[Fixup initialization paper](https://arxiv.org/abs/1901.09321)\n",
    "- good initialization means you don't need normalization layers\n",
    "\n",
    "But good initialization can help too.\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Initialization is like priming a pump: we want our gradients and learning to flow smoothly.\n",
    "\n",
    "Proper initialization not only facilitates learning, but may actually speed up training (e.g. Batch Normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
