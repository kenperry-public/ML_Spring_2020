{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "%\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN in action: Understanding sequences\n",
    "\n",
    "We will study a toy example that is typical of many tasks involving sequences\n",
    "- Given a prefix of a sequence\n",
    "- Predict the next element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "- Predict the next word in a sentence\n",
    "- Predict the next price in a timeseries of prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being able to predict the next element may be key to understanding the \"logic\" underlying a sequence\n",
    "- You have to understand context and domain\n",
    "- You have to understand how earlier elements influence latter elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Predict the next:  Data preparation\n",
    "\n",
    "It is our belief that Machine Learning is a *process* and not just a collection of models.\n",
    "\n",
    "We have recently been emphasizing the models but let's review the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Recipe for Machine Learning</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ML_process.jpg\"</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is usually the case that Sequence data involves substantial Data Preparation.\n",
    "\n",
    "Suppose our task is to predict the next word in a sentence.\n",
    "\n",
    "We are given (or must obtain) a collection of sentences (e.g., one or more documents) as our raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But a sentence is not the format required for the training set of the \"Predict the next word\" task.\n",
    "\n",
    "Data preparation is usually a substantial prerequisite for solving tasks involving sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be precises, the \"Predict the next word\" task involves\n",
    "- Training a many to one RNN with examples created from a sequence.\n",
    "- The elements of a single example are the prefix of a sentence\n",
    "- The target of the example is the next word in the sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let \n",
    "$$[ \\;\\mathbf{s}_\\tp | 1 \\le \\tt \\le T \\; ]$$\n",
    "be the sequence of words in sentence \\mathbf{s}.\n",
    "\n",
    "We will prepare $(T-1)$ examples from this single sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\\langle \\X, \\y \\rangle = $\n",
    "\n",
    "$\n",
    "\\begin{array} \\\\\n",
    "  i  & \\x^\\ip  & \\y^\\ip \\\\\n",
    "  \\hline \\\\\n",
    "  1 & \\mathbf{s}_{(1) }  & \\mathbf{s}_{(2)} \\\\\n",
    "  2 & \\mathbf{s}_{(1), (2) }  & \\mathbf{s}_{(3)} \\\\\n",
    "  \\vdots \\\\\n",
    "  i & \\mathbf{s}_{(1), \\ldots, (i) }  & \\mathbf{s}_{(i+1)} \\\\\n",
    "  \\vdots \\\\\n",
    "  (T-1) & \\mathbf{s}_{(1), \\ldots, (T-1) }  & \\mathbf{s}_{(T)} \\\\\n",
    "  \\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For example\n",
    "\n",
    "$\\mathbf{s} = $\n",
    "\"I am taking a class in Machine Learning\"\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{array}\\\\\n",
    "i & \\x^\\ip  & \\y^\\ip \\\\\n",
    "1 & [\\;  \\text{I} \\; ] & \\text{am} \\\\\n",
    "2 & [\\; \\text{I, am} \\; ] & \\text{taking} \\\\\n",
    "3 & [\\; \\text{I, am, taking} \\; ] & \\text{a} \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next: data shape\n",
    "\n",
    "We had warned earlier about the explosion of th enumber of dimensions of our data.\n",
    "Now is a good time to take stock\n",
    "- $\\X$, the training set, is a matrix with $m$ rows\n",
    "- Each row is an example $\\x^\\ip$\n",
    "- Each example is a sequence $[ \\; \\x^\\ip_\\tp \\, | \\, 1 \\le \\tt \\le || \\x^\\ip || \\; ]$\n",
    "- Each element $\\x^\\ip_\\tp$ of the sequence encodes a word\n",
    "- A word is encoded as a One Hot Encoded binary vector of length $|| V||$ where $V$ is the set of words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Target $\\y^\\ip$ is also a word (so is  vector of length $|| V||$).\n",
    "- Many to one: target is *not* a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predict the next: training\n",
    "\n",
    "Just like training any other type of layer, but more expensive\n",
    "- Each example involves multiple time steps: forward pass time consuming\n",
    "- The derivatives (needed for Gradient Descent) are more complex; backward pass complex and time consuming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a generative model (fun with RNN's)\n",
    "\n",
    "The \"Predict the next\" word task is interesting on its own\n",
    "- But a slight twist will make it extremely interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose\n",
    "- We train the RNN on a large number of sentences of the same type (e.g., same author)\n",
    "- Create a few words to create the prefix of a sentence\n",
    "- Ask the RNN to predict the next word\n",
    "- Append this word to the prefix\n",
    "- Repeat !\n",
    "\n",
    "Voila: the RNN can *generate* a story in the same style as the training sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using Machine Learning to *create* data is called *generative*.\n",
    "\n",
    "Using Machine Learning to classify/predict (as we've been doing thus far) is called *discriminative*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Architecture\n",
    "\n",
    "How do we construct a model to solve this task ?\n",
    "\n",
    "We construct a two part model in what is known as an *Encoder/Decoder* architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Encoder* is a many to one RNN\n",
    "- Takes the variable length \"seed\" sequence\n",
    "- Outputs a fixed length representation of the seed\n",
    "    - This is one of the strengths of an RNN\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The *Decoder* is a one to many RNN\n",
    "- Takes the fixed length representation of the seed produced by the Encoder\n",
    "    - Used to initialized the Decoder's latent state $\\h_{(0)}$\n",
    "- Outputs a variable length sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The only thing unusual about the Decoder is how its input sequence is constructed\n",
    "- All the elements of its input $\\x$ are *generated* by the Decoder\n",
    "- The first element of $\\x$ is set to a special \"start of output\" symbol\n",
    "$$\\x_{(1)} = \\langle \\text{START} \\rangle$$\n",
    "- $\\x_\\tp$ is extending *dynamically*, using the previous prediction $\\hat{\\y}_{(\\tt-1)}$\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "- The Decoder stops when it generates a special \"end of output\" symbol $\\langle \\text{END} \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: inference</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq_inference.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is\n",
    "- The output is fed as the next input\n",
    "- Thus extending the sequence\n",
    "- And making sure that subsequent elements are influenced by all previously *generated* elements\n",
    "- Continuing until the special $\\langle \\text{END} \\rangle$ symbol is generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training: Teacher forcing\n",
    "\n",
    "Some issues arise in using an RNN in the generative manner.\n",
    "\n",
    "The first issue:\n",
    "- Is the prediction a single word or a probability distribution over the vocabulary $||V||$\n",
    "- If it's a single word: the output is deterministic\n",
    "    - Problematic once one word is wrong: the error propagates forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output of the multinomial classifier is a vector of length $||V||$\n",
    "- With values in the range $[0,1]$ that can be interpretted as probabilities\n",
    "- Rather than choosing the single word with highest probability\n",
    "- We *sample* one word at random, according to the probability distribution\n",
    "\n",
    "This makes the output non-deterministic: running the model twice with the same \"seed\" may give different stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A second issue\n",
    "- If a wrong word is chosen at step $\\tt$: it affects the generation of all words at step $\\tt \\gt \\tt'$ \n",
    "- This is particularly problematic at *training* time: makes learning difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution for training an RNN for this task is a method known as *teacher forcing*\n",
    "- Rather than extending the seed example $\\x^\\ip = [ \\; \\x^\\ip_\\tp \\, | \\, 1 \\le \\tt \\le t' \\; ]$\n",
    "    - With $\\hat{\\y}_\\tp$, the *predicted* $\\tt^{th}$ word for $(\\tt > \\tt')$\n",
    "    - Which is what would happen at inference/test time\n",
    "    - Extend it with $\\y_\\tp$, the *target* (i.e., correct  $\\tt^{th}$ word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words: to speed up training\n",
    "- When extending the prefix\n",
    "- A teacher forces the student (model) to continue with the *correct* answer\n",
    "- Rather than the student's answer\n",
    "\n",
    "$$\n",
    "\\x^\\ip_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "for $t > t'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq_training.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Teacher forcing is indicated in red\n",
    "- Predictions $[ \\; \\hat{\\y}_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ **are not** used as input (lower right)\n",
    "- Only correct targets $[ \\; \\y_\\tp \\, | \\, 1 \\le \\tt \\le T \\; ]$ are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "Here is an unrolled graph at inference/test time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_inference.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And here is a depiction of the graph used at *training* time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training, with Teacher Forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_training.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating strange things\n",
    "\n",
    "Generating stories from seeds was very popular a few years back.\n",
    "\n",
    "Let's look at some examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But first, a surprise:\n",
    "- Rather than solving a \"predict the next word\" task\n",
    "- All of the following examples were generated by a \"predict the next **character**\" task !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is somewhat amazing that what is generated\n",
    "- Has correctly spelled words/keywords\n",
    "- Is Syntactically correct (sentences end with a \".\", parentheses/brackets are balanced)\n",
    "- Is meaningful: the elements/words are arranged in a logical order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even though\n",
    "- We have not explicilty identified any of these concepts\n",
    "- Nor forced training to respect them (via a loss function)\n",
    "\n",
    "Remember\n",
    "- All of this behavior was \"learned\" by identifying the correct next **character**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fake [Shakespeare](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare), or fake politician-speak\n",
    "- Fake code \n",
    "- Fake [math textbooks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#algebraic-geometry-latex)\n",
    "- [Click bait headline generator](http://clickotron.com/about)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
