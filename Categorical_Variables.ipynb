{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Common imports\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Categorical variables\n",
    "\n",
    "A categorical variable\n",
    "- Has a value drawn from a discrete set called *Categories* or *Classes*\n",
    "    - hence the term \"Classification\" when the target is categorical\n",
    "- There is **no** ordering relationship between category/class values\n",
    "    - { \"Red\", \"Green\", \"Blue\" }  (set notation)\n",
    "    - versus *ordinal* values [ \"Small\", \"Medium\", \"Large\" ] (sequence notation)\n",
    "- There is no *magnitude*\n",
    "    - even if I could order the colors: how much greater is \"Blue\" than \"Red\" ?            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will use $C$ to denote the set of possible values in a category/class.\n",
    "\n",
    "Since the values in $C$ are unordered, $C$ is mathematically a set of values\n",
    "$$\n",
    "C = \\{ c_1, c_2, \\ldots, \\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since values in a category/class aren't ordered, they are often non-numeric.\n",
    "\n",
    "Most algorithms deal with non-numeric data by encoding them as numeric.\n",
    "\n",
    "In our Titanic example for Binary Classification, there were two obvious categorical variables\n",
    "- `Survived` (the target)\n",
    "- `Sex`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It might have gone un-noticed that the target was categorical\n",
    "- Because\n",
    "the data was presented to us as having been encoded with number $1$ (Survived) and $0$ (didn't Survive)\n",
    "\n",
    "We certainly noticed that `Sex` was non-numeric\n",
    "- Hence, we created a transformation to encode Male/Female as 0/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if a categorical variable has more than 2 possible values for category/class ?\n",
    "\n",
    "An obvious choice for a variable with $||C|| >2$ is to encode the values with distinct integers\n",
    "\n",
    "This is usually a **bad** idea !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the `Pclass` feature from our Titanic example.\n",
    "\n",
    "It was presented to us as a numeric feature (integers in $\\{ 1, 2, 3\\}$)\n",
    "so we may have taken for granted that `Pclass` was **not** categorical.\n",
    "\n",
    "But it could have just as easily been present as non-numeric  { \"First\", \"Second\", \"Third\" }.\n",
    "\n",
    "- If not for coincidence, perhaps `Pclass` should be considered a *categorical* rather than *ordinal* feature ?\n",
    "\n",
    "- Why is encoding the strings as $\\{ 1, 2, 3\\}$ any more correct than encoding them as $\\{ 1, 2, 4 \\}$ ?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will give a fuller answer in the module on Model Interpretation.  For now:\n",
    "\n",
    "- In a linear model\n",
    "    $$\\y = \\Theta^T \\cdot \\x$$\n",
    "    - Thus, the contribution of the $j^{th}$ feature $\\x_j$ to prediction $\\y$ is $\\Theta_j * \\x_j$\n",
    " \n",
    "- Consider the encoding of $\\x_j$ (`Pclass`) as $\\{ 1, 2, 3 \\}$\n",
    "    - The difference in contribution betwen \"First\", \"Second\" and \"Third\" are all equal\n",
    "- Consider the encoding of $\\x_j$ (`Pclass`) as $\\{ 1, 2, 4 \\}$    \n",
    "   - The difference in contribution betwen \"Second\" and \"Third\" is twice that of \"First\" and \"Second\"\n",
    "\n",
    "The arbitrary choice of encoding may have an impact on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Bottom line**\n",
    "- Consider whether a feature should be treated as categorical *regardless* of the encoding presented\n",
    "- Arbitary mapping of a categorical value to an integer has consequences\n",
    "    - Avoid it !\n",
    "    \n",
    "We will describe the proper way to encode categorical variables\n",
    "- And revisit the Titanic example, changing `Pclass` to categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One hot encoding (OHE)\n",
    "\n",
    "Categorical variables can have more than 2 classes.\n",
    "\n",
    "The way to represent a categorical variable $v$ with $||C||$ classes is with a \n",
    "vector $\\v$ of length $||C||$.\n",
    "\n",
    "$\\v$ will have the property of being all zero *except* at a single index $j$ where $\\v_j = 1$.\n",
    "\n",
    "Hence the name *one hot* encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to create a mapping $m$ from class $c \\in C$ to  integers $\\in \\left[ \\, 1, ||C|| \\,\\right]$.\n",
    "\n",
    "This will enable us to associate an integer with a class label.\n",
    "\n",
    "For example $i$, suppose $v$ has class label $c$.\n",
    "\n",
    "Then $\\v^\\ip$ is defined as\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\v^\\ip_j & = & 1 & \\text{ if } j = m(c) \\\\\n",
    "\\v^\\ip_j & = & 0 & \\text{ if } j \\ne m(c) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**n.b., we may slip into writing $\\mathbf{v}^\\ip_c$ rather than $\\mathbf{v}^\\ip_{m(c)}$**\n",
    "- since $c$ is a category (non-numeric) and $m(c)$ is an integer this is unambgious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The categorical variable $v$ was replaced with $||C||$ binary variables $\\v_1, \\ldots, \\v_{||C||}$.\n",
    "- for each example $i$: there is exactly $1$ index $j$ such that $\\v_j = 1$\n",
    "- if feature $\\v$ of example $i$ was from the $j^{th}$ class in $C$, then $\\v^\\ip_j = 1$\n",
    "\n",
    "Each of the new *binary* features is called an *indicator* or *dummy* variable.\n",
    "\n",
    "\n",
    "This is called **one hot encoding (OHE)** of a variable.\n",
    "\n",
    "We can use OHE on variables, whether they are targets or features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's see what $\\v$ looks like on a number of examples, one for each possible class $c \\in C$:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\\\\\n",
    "&    & \\v_1 & \\v_2 & \\v_3 & \\ldots & \\v_{||C||} \\\\\n",
    "&m(c)=1 & 1    & 0    & 0    &        & 0    \\\\\n",
    "&m(c)=2 & 0    & 1    & 0    &        & 0    \\\\\n",
    "&m(c)=3 & 0    & 0    & 1    &        & 0    \\\\\n",
    "& \\ldots \\\\\n",
    "&m(c)=||C|| & 0    & 0    & 0    &        & 1    \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be concrete let's consider a data set with\n",
    "- one numeric variable $x_0$\n",
    "- categorical variable $x_1$ (\"Gender\") from categories $C_{(1)} = \\{ \"Male\", \"Female\" \\}$\n",
    "- categorical variable $x_2$ (\"Location\")from categories $C_{(2)} = \\{ \"Urban\", \"Suburban\", \"Exurban\" \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And a few rows from our data set\n",
    "\n",
    "$\n",
    "  \\X' = \\begin{pmatrix}\n",
    "  \\mathbf{const} & \\mathbf{Gender} & \\mathbf{Location} \\\\\n",
    "  1 & Female & Urban \\\\ \n",
    "  1 & Female & Exurban \\\\ \n",
    "  1 & Male & Exurban \\\\ \n",
    "  1 & Male & Suburban \\\\ \n",
    "   \\vdots \\\\\n",
    "  \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After our One Hot Encoding we get\n",
    "\n",
    "$\n",
    "  \\X'' = \\begin{pmatrix}\n",
    "  \\mathbf{const} & \\mathbf{Is Female} & \\mathbf{Is Male} & \\mathbf{Is Urban} & \\mathbf{Is Suburban} & \\mathbf{Is Exurban}\\\\\n",
    "  1 &  1 & 0 & 1 & 0 & 0 \\\\ \n",
    "  1 &  1 & 0 &  0 & 0 & 1\\\\ \n",
    "  1 &  0 & 1 &  0 & 0 & 1 \\\\ \n",
    "  1 &  0 & 1 &  0 & 1 & 0 \\\\ \n",
    "   \\vdots \\\\\n",
    "  \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that the number of features has increased.\n",
    "\n",
    "Specifically: a single categorical variable $x_j$ from a category $C = \\{ c_1, c_2, \\ldots \\}$ of size $||C||$ \n",
    "- has been replaced by $||C||$ new variables\n",
    "   - $\\text{Is } c_1$\n",
    "   - $\\text{Is } c_2$\n",
    "   - $\\vdots$\n",
    "   - $\\text{Is } c_{||C||}$\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Categorical features versus categorical targets\n",
    "\n",
    "OHE can be applied to a variable, regardless of whether the variable is a feature ($\\x$) or a targer ($\\y$).\n",
    "\n",
    "There are some subtle differences in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Categorical targets\n",
    "\n",
    "Although we should use OHE to encode the targets, *in practice* you might see targets encoded as integers\n",
    "- Binary targets as 0/1\n",
    "- Other targets as integers\n",
    "    - `sklearn` method `LabelEncoder` does exactly this\n",
    "\n",
    "If it's such a bad idea: why does this happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The answer\n",
    "- *Mathematically* it is a bad idea\n",
    "- It **may** not matter *from a coding perspective*\n",
    "    - Often, the code need only be able to *distinguish between* target values\n",
    "        - e.g., restrict the examples to those with a particular value of the target\n",
    "    - So the encoding of values is not important\n",
    "    - In fact: `sklearn` is perfectly happy with non-numeric targets for just this reason !\n",
    "    - It will matter in the Deep Learning part of the course\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Bottom line**\n",
    "- You will see distinct integer (and string) encodings of targets in `sklearn`\n",
    "- When discussing the mathematics of Loss functions\n",
    "we will represent discrete $\\y^\\ip$ by a *specific encoding*\n",
    "    - for Binary Classification:\n",
    "        - by either $0/1$ or $-1/+1$\n",
    "    - for Multinomial Classification (number of classes $||C|| > 2$)\n",
    "        - by a *vector* of $0$'s and $1$'s of length $||C||$\n",
    "        - *One Hot Encoding* (OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So please be aware that when we encode categorical targets \n",
    "- it is for a mathematical formula\n",
    "- and not *necessarily* a pre-processing step you must perform on the examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Categorical features\n",
    "\n",
    "Much as we would like to have a Machine Learning universe in which everything was uniform\n",
    "- there is one model in which OHE may cause a problem\n",
    "    - Linear Regression, with an intercept\n",
    "    - There is a simple fix (i.e., an argument to pass to implementations of OHE)\n",
    "\n",
    "The issue is called the *Dummy variable* trap and will be explained in a Deep Dive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text: another categorial variables\n",
    "\n",
    "How do you include text variables ? One-hot encoding of the vocabulary !\n",
    "\n",
    "That's only approximately true, as vocabularies can be quite large and thus, the vectors are very long.\n",
    "\n",
    "Dealing with Text will be the subject of another lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:** Spam filtering (classification task: Is Spam/Is Not Spam)\n",
    "- each word is an indicator\n",
    "- Dealing with large vocabulary\n",
    "    - sparse matrices\n",
    "    - word vectors\n",
    "- feature engineering: an ALLCAP feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "- We introduced methods to deal with non-numeric variables\n",
    "- Unfortunately, there are some nuances\n",
    "\n",
    "Let's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "368px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
