{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "%\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro\n",
    "\n",
    "In Classical ML, the paradigm was\n",
    "- construct, by hand, transformations of the input to alternatate representations\n",
    "    - feature engineering: create representations corresponding to a \"concept\" useful for classification\n",
    "    - we called this a pipeline\n",
    "- after multiple transformations, the representation was good enough that a classifer could separate classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Deep Learning, the paradigm is very similar\n",
    "- a sequence of transformations\n",
    "    - each transformation is called a \"layer\"\n",
    "        - \"Deep Learning\" is many layers of transformation\n",
    "    - each layer successively constructs a new representation\n",
    "    \n",
    "**The key difference from Classical ML**:\n",
    "- the transformations are \"discovered\"  rather than hand engineered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning: the cartoon\n",
    "Here is a \"cartoon\" diagram of Deep Learning (as applied to Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <center>Layers</center>\n",
    "    <br>\n",
    "<img src=images/NN_Layers.jpg width=2500>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a sequence of *layers* (vertical boxes)\n",
    "- It starts with the input layer (the large vertical box on the left)\n",
    "- Layer $\\ll$ takes as input the output of layer $(\\ll -1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of the penultimate layer $(L-1)$\n",
    "- Is used as input to a final layer $L$ that implements\n",
    "    - Classifier for Classification\n",
    "    - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The process of moving through the layers from Input to penultimate is\n",
    "- Successive transformation of the input\n",
    "- Each layer's output is an alternate *representation* of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is just a Transformation Pipeline (like we've seen in `sklearn`)\n",
    "- Where the final version of the transformed data is fed into a Classifier/Regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we will come to see:\n",
    "- The **key difference** from Classical Machine Learning\n",
    "- Is that the **transformations are non-linear**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our old friend, the dot product, will play a starring role.\n",
    "- it is a fundamental part of many layers that we will study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with our the inputs to the NN: an example with our original features\n",
    "\n",
    "Recall that the dot product can be thought of implementing a kind of \"pattern matching:\n",
    "- looking for a subset of features in an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So one way to think of the transformations implemented by the **first layer** is that it\n",
    "- Recognizes patterns in the $n$ original features\n",
    "- Constructs $n_{(1)}$ new synthetic features\n",
    "\n",
    "Layer $\\ll$\n",
    "- Recognizes patterns in the $n_{(\\ll-1)}$ synthetic features created by layer $(\\ll-1)$\n",
    "- Constructs $n_\\llp$ new synthetic features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ultimately, the $n_{(L-1)}$ synthetic features created by penultimate layer $(L-1)$\n",
    "- Is a representation of the input $\\x$\n",
    "- That has been sufficiently transfored\n",
    "- So that a Classifier/Regression model at layer $L$ can be successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks: introduction and notation\n",
    "\n",
    "The above was very informal.\n",
    "\n",
    "We need to introduce more concepts and, unfortunately, the notation that will carry us through\n",
    "the Deep Learning part of the course.\n",
    "\n",
    "Time to go [under the covers of a layer](Intro_to_Neural_Networks.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is $W_\\llp$ ? Where did $\\Theta$ go ?\n",
    "\n",
    "Our old friend the dot product is back in the forefront.\n",
    "\n",
    "But now, the pattern matching is written\n",
    "$$\n",
    "\\W_{\\llp,j} \\cdot \\y_{(\\ll-1)}\n",
    "$$\n",
    "rather than the \n",
    "$$\n",
    "\\Theta \\cdot \\x\n",
    "$$\n",
    "which was familiar in the Classical Machine Learning part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Unfortunately, this is an artifact of two different communities working independently\n",
    "- The Classical Machine learning community uses the term *parameters* and the Greek letter $\\Theta$\n",
    "- The Computer Science community uses the term *weights* and the letter $W$\n",
    "\n",
    "They are *exactly the same thing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, Neural Networks operate in layers\n",
    "- So only the dot product of the *first* layer involves $\\x$, which we have equated with $\\y_{(0)}$\n",
    "- The dot product of layer $\\ll$ is finding patterns in $\\y_{(\\ll-1)}$ rather than $\\x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Size of $\\W$: Always count the number of parameters !\n",
    "\n",
    "When constructing a layer of a Neural Network: **always count the number of weights/parameters. They grow quickly !!**\n",
    "\n",
    "$\\W_\\llp$\n",
    "- Consists of $n_\\llp = || \\y_\\llp ||$ units, each unit producing a new feature\n",
    "- Each unit performs the dot product \n",
    "$$\n",
    "\\y_{\\llp,j} = \\y_{(\\ll-1)} \\cdot \\W_{\\llp,j}\n",
    "$$\n",
    "- Each dot product thus involes $|| \\y_{(\\ll-1)} || +1 $ weights in $\\W_{\\llp,j}$\n",
    "    - the \"+ 1\" is because of the bias term in each unit\n",
    "- Thus the number of weights in $\\W_\\llp$ is $|| \\y_\\llp || * (|| \\y_{(\\ll-1)} || + 1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does a layer do ?\n",
    "\n",
    "Surprisingly: this is not an easy question to answer !\n",
    "\n",
    "The behavior of Neural Networks is a bit mysterious.\n",
    "\n",
    "We will spend part of a future lecture on trying to interpret what is happening inside the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now, one can imagine that\n",
    "- The first layer recognizes features (matches patterns) for *primitive* concepts\n",
    "- The second layer recognizes features that are *combinations* of primitive concepts (layer 1 concepts)\n",
    "- The $\\ll$ recognizes features that are *combinations* of layer $(\\ll-1)$ concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center>Features by layer</center>\n",
    "    <br>\n",
    "<img src=images/Layer_features.jpg>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activation functions\n",
    "\n",
    "At this point perhaps you have a mechanical understanding of a neural network\n",
    "- A sequence of layers\n",
    "- Each layer is creating new features\n",
    "- Subsequent layers creating features of increasing complexity but transforming the prior layer's features\n",
    "- A new feature is created by a liner dot product followed by an non-linear activaton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It turns out that the non-linear activation function is *one of the keys* to Neural Networks !\n",
    "\n",
    "Let's explore [Activation functions](Neural_Networks_Activations.ipynb) in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final layer: Regression/Classification\n",
    "\n",
    "Layer $L$ in our cartoon implements Regression or Classification.\n",
    "- Regression is nothing more than a dot product followed by an identity/linear activation\n",
    "$$\n",
    "\\y_{L} = \\y_{(L-1)} \\cdot \\W_{L}\n",
    "$$\n",
    "- Classification is nothing more than a dot product followed by a sigmoid activation\n",
    "$$\n",
    "\\y_{L} = \\sigma( \\y_{(L-1)} \\cdot \\W_{L} )\n",
    "$$\n",
    "as discussed in our lecture on Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions to consider\n",
    "\n",
    "Some natural questions to ask at this point\n",
    "- How many layers should we have ( What is the right value for $L$) ?\n",
    "- How many units $n_\\llp$ should I have for each layer $1 \\le \\ll \\le (L-1)$ ?\n",
    "- What activation function should I use for each unit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will address each of these in the future.\n",
    "\n",
    "Perhaps the biggest question\n",
    "- $\\W_{\\llp,j}$ is the pattern used to recognize the feature created by unit $j$ of layer $\\ll$\n",
    "- How does $\\W_{\\llp,j}$ get set ?\n",
    "\n",
    "This will be the topic of the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training a Neural Network\n",
    "\n",
    "We will start to answer the question of how $\\W$ is determined.\n",
    "\n",
    "\n",
    "We will briefly [introduce training a Neural Network](Neural_Networks_Intro_to_Training.ipynb),\n",
    "a subject we will revisit in-depth in a later lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tensorflow: A toolkit for Neural Networks\n",
    "\n",
    "Why do we need a dedicated toolkit (Tensorflow) to aid the programming of Neural Networks ?\n",
    "\n",
    "It's mainly about the use of Gradient Descent in training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall that a Neural Net (including one augmented by a Loss Layer) is doing nothing more than computing a function.\n",
    "\n",
    "Gradient Descent needs to take the gradient of this function (evaluated on a minibatch of examples) in order to update the weights $\\W$.\n",
    "\n",
    "There are at least two ways to obtain the Gradient\n",
    "- Numerically\n",
    "- Analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numerical differentiation applies the mathematical definition of the gradient\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\frac{ f(x + \\epsilon) - f(x) }{\\epsilon}\n",
    "$$\n",
    "\n",
    "- It evalues the function twice: at $f(x)$ and $f(x+\\epsilon)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This can get expensive, especially since\n",
    "    - $\\x$ is a vector\n",
    "    - Potentially very long (e.g., many features)\n",
    "    - We need to evaluate the derivative of $f(\\x)$ with respect to each $\\x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Analytic derivates are how you learned differentiation in school\n",
    "- As a collection of rules, e.g.,\n",
    "$$\n",
    "\\frac{\\partial (a + b)}{\\partial x} = \\frac{\\partial a}{\\partial x} + \\frac{\\partial b}{\\partial x}\n",
    "$$\n",
    "\n",
    "This is very efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue in ordinary  code\n",
    "- The expression `(a + b) * c`\n",
    "- Is evaluated `tmp = a + b`\n",
    "- And the result passed to the next step of the computation, e.g, `tmp * c`\n",
    "- Losing the connection between `tmp` (a value) and the operation (plus) and addends (`a`, `b`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is no information recorded in ordinary code that would allow the application of analytic rules of differentiation.\n",
    "\n",
    "Tensorflow is different in that `(a + b) * c`\n",
    "- Is a symbolic expression (i.e., recorded as operation and arguments)\n",
    "- That is saved\n",
    "- Facilitating the application of analytic rules of differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We still write `(a + b) * c` but it really results in something like:\n",
    "- `tf.math.mult( tf.math.add(a, b), c)`\n",
    "- The expression `tf.math.add(a, b)`\n",
    "    - Can be differentiated analytically\n",
    "    - Since we know the operation is addition\n",
    "    - It records that the arguments are `a, b`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So Tensorflow facilitates analytic function differentiation while hiding the details from the user.\n",
    "\n",
    "By the way: what is a Tensor ?  It is an object with an arbitrary number of dimensions.\n",
    "\n",
    "We use special cases all the time:\n",
    "- A scalar is a tensor of dimension $0$\n",
    "- A vector is a tensor of dimension $1$\n",
    "- A matrix is a tensor of dimension $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you've seem, we are already dealing with higher dimensional objects.\n",
    "\n",
    "Consider $\\y$:\n",
    "- $\\y_{\\llp, j, j'}$\n",
    "    - Output of layer $\\ll: \\y_\\llp$\n",
    "    - Unit $j$ of layer $\\ll: \\y_{\\llp, j}$\n",
    "    - Element $j'$ of the output of unit $j$ of layer $\\ll: \\y_{\\llp, j, j'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the future we will talk about *sequences* of $\\y$, thus adding another dimension: time.\n",
    "\n",
    "The notation will become a little heavy but hopefully understandable as a way of indexing a high dimension object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequential versus Functional construction of a Neural Network\n",
    "\n",
    "The above description of a Neural Network imposed several restrictions on units (neurons)\n",
    "- Each layer is homogeneous\n",
    "- Layers are organized sequentially\n",
    "\n",
    "One can imagine a network graph without these restrictions\n",
    "- Not organized as layers\n",
    "- Pairs of units with arbitrary connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center>Sequential architecture</center>\n",
    "    <br>\n",
    "<img src=images/Sequential_arch.jpg width=400>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center>Functional architecture</center>\n",
    "    <br>\n",
    "<img src=images/Functional_arch.jpg width=400>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The layer-oriented network we described is called *Sequential* in Keras.\n",
    "\n",
    "The unrestricted network form is called *Functional* in Keras.\n",
    "\n",
    "For the most part, we will use Sequential networks\n",
    "- Easier to build\n",
    "- But less flexible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Some \"Why's ?\"\n",
    "\n",
    "## What took so long: Preview\n",
    "\n",
    "An historical perspective:\n",
    "\n",
    "- Perceptron invented 1957\n",
    "- mid-1970's: First \"AI Winter\"\n",
    "- Late 1980's: secibd \"AI Winter\"\n",
    "- 2010: Re-emergence of AI\n",
    "\n",
    "The promise of AI led to great expectations, that were ultimately unfulfilled.\n",
    "The difficulty was the inability to train networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will defer a fuller answer to a later lecture.\n",
    "\n",
    "For now: seemingly minor choices were more impactful than imagined\n",
    "- Sigmoid as activation function turned out to be a problematic choice\n",
    "- Initializing $\\W$ properly was more important than imagined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Vanishing/Exploding Gradients\n",
    "    - problems arise when the gradient is effectively  0\n",
    "    - problems also occurs when they are effectively infinite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Computational limits\n",
    "    - It turns out to be quite important to make your NN big; bigger/faster machines help\n",
    "    - Actually: bigger than it needs to be\n",
    "        - many weights wind up near $0$, which renders the neurons useless\n",
    "        - [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n",
    "            - within a large network is a smaller, easily trained network\n",
    "            - increasing network side increases the chance of large network containing a trainable subset\n",
    "            - [summary](https://towardsdatascience.com/how-the-lottery-ticket-hypothesis-is-challenging-everything-we-knew-about-training-neural-networks-e56da4b0da27)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why do GPU's matter ?\n",
    "\n",
    "[What makes TPU's fined tuned for Deep Learning](https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning)\n",
    "\n",
    "GPU (Graphics Processing Unit): specially designed hardware to perform repeated\n",
    "vector multiplications (a typical calculation in graphics processing).\n",
    "\n",
    "- It is not general purpose (like a CPU) but does what it does extremely quickly, and using many\n",
    "more cores than a CPU (typically several thousand).\n",
    "\n",
    "- As matrix multiplication is a fundamental operation of Deep Learning, GPU's have the ability to greatly\n",
    "speed up training (and inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Google has a further enhancement called a TPU (Tensor Processing Unit) to speed both training and inference.\n",
    "-  highly specialized to eliminate bottlenecks (e.g., memory access) in fundamental Deep Learning matrix multiplication.\n",
    "\n",
    "Both GPU's and TPU's \n",
    "- Incur an overhead (a \"set up\" step is needed before calculation).\n",
    "- So speedup only for sufficiently large matrices, or long \"calculation pipelines\" (multiplying \n",
    "different examples by the same weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "DL involves \n",
    "- Multiplying large matrices (each example) \n",
    "- By large matrices (weights, which are same for each example in batch)\n",
    "- Both GPU's and TPU's offer the possibility of large speed ups.\n",
    "\n",
    "- GPU's are **not** necessary\n",
    "    - but they are a **lot** faster\n",
    "    - life changing experience\n",
    "        - 30x faster means your 10 minute run (that ended in a bug) now only takes 20 seconds\n",
    "     - increases your ambition by faster iteration of experimental cycle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
